<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="ko" >
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >
  <!-- MathJax 추가 -->
  <script type="text/javascript" async
  src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Transformer Algorithm" />
<meta name="author" content="Patrick" />
<meta property="og:locale" content="ko" />
<meta name="description" content="1. Transformer Model {% include figure.html path=&quot;assets/img/nlp/Pasted%20image%2020240105144425.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; zoomable=true %}" />
<meta property="og:description" content="1. Transformer Model {% include figure.html path=&quot;assets/img/nlp/Pasted%20image%2020240105144425.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; zoomable=true %}" />
<link rel="canonical" href="http://localhost:4000/posts/Transformer/" />
<meta property="og:url" content="http://localhost:4000/posts/Transformer/" />
<meta property="og:site_name" content="Patrick’s Dev. Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-20T18:18:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Transformer Algorithm" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="twitter:creator" content="@Patrick" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Patrick"},"dateModified":"2023-12-20T18:18:00+09:00","datePublished":"2023-12-20T18:18:00+09:00","description":"1. Transformer Model {% include figure.html path=&quot;assets/img/nlp/Pasted%20image%2020240105144425.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; zoomable=true %}","headline":"Transformer Algorithm","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/Transformer/"},"url":"http://localhost:4000/posts/Transformer/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Transformer Algorithm | Patrick's Dev. Blog
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

  <link rel="manifest" href="/assets/img/favicons/site.webmanifest">

<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Patrick's Dev. Blog">
<meta name="application-name" content="Patrick's Dev. Blog">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Resource Hints -->
  
    
      
        <link rel="preconnect" href="https://fonts.googleapis.com" >
      
        <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
      
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      
        <link rel="dns-prefetch" href="https://fonts.gstatic.com" >
      
    
      
        <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      
        <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
      
    
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
  

  <!-- Scripts -->

  <script src="/assets/js/dist/theme.min.js"></script>

  <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script>







<script defer src="/assets/js/dist/post.min.js"></script>



<!-- Pageviews -->

  

  



  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"><img src="https://chirpy-img.netlify.app/assets/img/profile.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <a class="site-title d-block" href="/">Patrick's Dev. Blog</a>
    <p class="site-subtitle fst-italic mb-0">이것 저것 해보는 중입니다</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/roomylot60"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/twitter_username"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['min2max.dev','google.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">Home</a>
            </span>

          
        
          
        
          
            
              <span>Transformer Algorithm</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link" aria-label="Search">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->





<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  




<!-- return -->










<article class="px-1" data-toc="true">
  <header>
    <h1 data-toc-skip>Transformer Algorithm</h1>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1703063880"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Dec 20, 2023
</time>

      </span>

      <!-- lastmod date -->
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              
                
                
              
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="3092 words"
>
  <em>17 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  
    <div id="toc-bar" class="d-flex align-items-center justify-content-between invisible">
      <span class="label text-truncate">Transformer Algorithm</span>
      <button type="button" class="toc-trigger btn me-1">
        <i class="fa-solid fa-list-ul fa-fw"></i>
      </button>
    </div>

    <button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm">
      <span class="label ps-2 pe-1">Contents</span>
      <i class="fa-solid fa-angle-right fa-fw"></i>
    </button>

    <dialog id="toc-popup" class="p-0">
      <div class="header d-flex flex-row align-items-center justify-content-between">
        <div class="label text-truncate py-2 ms-4">Transformer Algorithm</div>
        <button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75">
          <i class="fas fa-close"></i>
        </button>
      </div>
      <div id="toc-popup-content" class="px-4 py-3 pb-4"></div>
    </dialog>
  

  <div class="content">
    <h2 id="1-transformer-model"><span class="me-2">1. Transformer Model</span><a href="#1-transformer-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240105144425.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<ul>
  <li>2017년 Google에서 발표한 “Attention is all you need” 논문에서 제시한 모델</li>
  <li><strong>Attention Machanism</strong> 만을 사용하여 seq2seq의 구조인 <em>encoder-decoder</em>로 구현
    <ul>
      <li>seq2seq : Encoder, Decoder에서 각각 하나의 RNN이 t 개의 시점을 가지는 구조</li>
      <li>transformer : Encoder, Decoder 단위(Layer)가 N 개로 구성되는 구조</li>
    </ul>
  </li>
  <li>RNN을 사용하지 않았음에도 성능적인 우수성
    <ul>
      <li>Positional information : RNN은 단어의 위치에 따라 단어를 순차적(Sequential) 처리를 하여 단어의 위치 정보를 보유</li>
      <li>Positional Encoding : Transformer는 단어의 위치 정보(sin, cos)를 각 단어의 Embedding vector에 더하여 Model의 입력으로 사용</li>
    </ul>
  </li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240104161019.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240104162143.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
		<span class="nf">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
		<span class="n">self</span><span class="p">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

	<span class="c1"># position encoding에서 사용할 sin, cos의 각도 지정 함수
</span>	<span class="k">def</span> <span class="nf">get_angles</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
		<span class="n">angles</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">tf</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">position</span> <span class="o">*</span> <span class="n">angles</span>
	
	<span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
		<span class="n">angle_rads</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_angles</span><span class="p">(</span>
			<span class="n">position</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)[:,</span> <span class="n">tf</span><span class="p">.</span><span class="n">newaxis</span><span class="p">],</span>
			<span class="n">i</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)[</span><span class="n">tf</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span>
			<span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">)</span>
		
		<span class="c1"># Even : 2i
</span>		<span class="n">sines</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
		
		<span class="c1"># Odd : 2i+1
</span>		<span class="n">cosines</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
	
		<span class="n">angle_rads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># 홀수, 짝수에 따라 나누어진 각도들에 대한 정보를 통합
</span>		<span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">sines</span>
		<span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">cosines</span>
		<span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">)</span>
		<span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">pos_encoding</span><span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">...]</span>
		<span class="nf">print</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
		
		<span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</code></div></div>

<h3 id="seq2seq-model의-문제"><span class="me-2">seq2seq Model의 문제</span><a href="#seq2seq-model의-문제" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<ul>
  <li>Input sequence를 하나의 벡터(context vector)로 압축하는 과정에서 정보 손실 발생</li>
</ul>

<h3 id="hyper-parameter-in-transformer"><span class="me-2">Hyper-parameter in Transformer</span><a href="#hyper-parameter-in-transformer" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<ol>
  <li>d_{model} : Encoder, Decoder, Embedding vector에서의 차원</li>
  <li>num_layers : Layer(Encoder+Decoder)의 층 수</li>
  <li>num_heads : Transformer에서 Attention을 사용할 때 분할 및 병렬 수행, 결과값을 통합하는 방식을 사용하는데, 이 때의 병렬 수</li>
  <li>d_{ff} : Transformer 내부에 존재하는 Feed Forward Neural Network의 크기(이 때의 FFNN의 입출력층의 크기는 d_{model})</li>
</ol>

<h3 id="attention-in-tm"><span class="me-2">Attention in TM</span><a href="#attention-in-tm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<ol>
  <li>Encoder Self-Attention</li>
  <li>Masked Decoder Self-Attention</li>
  <li>Encoder-Decoder Attention</li>
</ol>

<hr />

<h2 id="2-encoder"><span class="me-2">2. Encoder</span><a href="#2-encoder" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<h3 id="first-sublayer--multi-head-self-attention"><span class="me-2">First sublayer : Multi-head Self-Attention</span><a href="#first-sublayer--multi-head-self-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<h4 id="self-attention"><span class="me-2">Self-Attention</span><a href="#self-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<ul>
  <li>Attention : Query에 대해서, Key와의 유사도를 mapping된 각각의 Value에 반영, Value의 가중합을 return</li>
</ul>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="n">Q</span> <span class="o">=</span> <span class="n">Query</span> <span class="c1"># t 시점의 decoder cell에서의 hidden state = t 시점의 encoder에서의 context vector
</span><span class="n">K</span> <span class="o">=</span> <span class="n">Keys</span> <span class="c1"># 모든 시점의 encoder cell의 hidden states = dot product의 대상이 되는 set
</span><span class="n">V</span> <span class="o">=</span> <span class="n">Values</span> <span class="c1"># 모든 시점의 encoder cell의 hidden states = weight과 곱해지는 vector의 set
</span></code></div></div>

<ul>
  <li>Self-Attention : Q, K, V가 모두 입력 문장의 모든 단어 벡터들을 의미
    <ul>
      <li>d_{model}의 차원을 갖는 단어 벡터들을 num_heads로 나눈 값을 Q, K, V의 벡터의 차원으로 결정</li>
    </ul>
  </li>
  <li>Scaled dot-product Attention : 내적만을 사용하는 Attention Function에 대해서 특정값 √n으로 나누어 scaling 하여 각 벡터의 모음; 행렬에 대해 연산하여 일괄 계산<code class="language-plaintext highlighter-rouge">score(q, k) = q · k / √n (n = d_{model} / num_heads = 각 Q, K, V의 차원값 d_{k})</code>
    <ul>
      <li>Padding Mask : Scaled dot-product attention에서 <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> token은 실질적인 의미를 갖지 않는 단어이므로, 이를 제외하기 위해 매우 작은 음수를 넣어 유사도에 반영되는 것을 막는 연산<br /></li>
    </ul>
  </li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240104175031.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240104175125.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
  <span class="c1"># query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
</span>  <span class="c1"># key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)
</span>  <span class="c1"># value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)
</span>  <span class="c1"># padding_mask : (batch_size, 1, 1, key의 문장 길이)
</span>
  <span class="c1"># Q와 K의 곱. 어텐션 스코어 행렬.
</span>  <span class="n">matmul_qk</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># 스케일링
</span>  <span class="c1"># dk의 루트값으로 나눠준다.
</span>  <span class="n">depth</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">key</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">matmul_qk</span> <span class="o">/</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span>

  <span class="c1"># 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.
</span>  <span class="c1"># 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.
</span>  <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

  <span class="c1"># 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.
</span>  <span class="c1"># attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)
</span>  <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="c1"># output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
</span>  <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</code></div></div>

<ul>
  <li>Multi-head Attention : Self Attention을 병렬적으로 사용하여 각각의 Attention head가 갖는 가중치 행렬이 다르게 설정;<br /></li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240104180009.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<p>여러 시점으로 정보를 수집하여 sequence를 구성하는 단어들간의 연관도를 측정<br /></p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240104180133.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="c1"># Multi-head Attention Implementation
# 각 Q, K, V를 만들기 위한 가중치 행렬과 결합 후에 사용하는 가중치 행렬 생성
# 구현 시에 입력을 전결합층; 밀집층(Dense layer)을 통과해 구현
</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">multi_head_attention</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

    <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># d_model을 num_heads로 나눈 값.
</span>    <span class="c1"># 논문 기준 : 64
</span>    <span class="n">self</span><span class="p">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span>

    <span class="c1"># WQ, WK, WV에 해당하는 밀집층 정의
</span>    <span class="n">self</span><span class="p">.</span><span class="n">query_dense</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">key_dense</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">value_dense</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># WO에 해당하는 밀집층 정의
</span>    <span class="n">self</span><span class="p">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">d_model</span><span class="p">)</span>

  <span class="c1"># num_heads 개수만큼 q, k, v를 split하는 함수
</span>  <span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">depth</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">query</span><span class="sh">'</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">key</span><span class="sh">'</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span>
        <span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># 1. WQ, WK, WV에 해당하는 밀집층 지나기
</span>    <span class="c1"># q : (batch_size, query의 문장 길이, d_model)
</span>    <span class="c1"># k : (batch_size, key의 문장 길이, d_model)
</span>    <span class="c1"># v : (batch_size, value의 문장 길이, d_model)
</span>    <span class="c1"># 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.
</span>    <span class="n">query</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query_dense</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key_dense</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value_dense</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="c1"># 2. 헤드 나누기
</span>    <span class="c1"># q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
</span>    <span class="c1"># k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)
</span>    <span class="c1"># v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)
</span>    <span class="n">query</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">split_heads</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">split_heads</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">split_heads</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="c1"># 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.
</span>    <span class="c1"># (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
</span>    <span class="n">scaled_attention</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
    <span class="c1"># (batch_size, query의 문장 길이, num_heads, d_model/num_heads)
</span>    <span class="n">scaled_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

    <span class="c1"># 4. 헤드 연결(concatenate)하기
</span>    <span class="c1"># (batch_size, query의 문장 길이, d_model)
</span>    <span class="n">concat_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span>
                                  <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">))</span>

    <span class="c1"># 5. WO에 해당하는 밀집층 지나기
</span>    <span class="c1"># (batch_size, query의 문장 길이, d_model)
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dense</span><span class="p">(</span><span class="n">concat_attention</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></div></div>

<h3 id="second-sublayer--ffnn"><span class="me-2">Second sublayer : FFNN</span><a href="#second-sublayer--ffnn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<ul>
  <li>Residual connection(잔차 연결) : Sublayer의 입력과 출력을 합(같은 차원을 가지므로 가능)을 구하는 과정 <a href="https://arxiv.org/pdf/1512.03385.pdf">입력과 출력의 합과 모델의 학습 관련 논문</a></li>
  <li>Layer Normalization(층 정규화) : Tensor의 마지막 차원; d_{model} 차원에 대해서 평균과 분산을 구하고, 이를 가지고 정규화하여 학습에 활용 <a href="https://arxiv.org/pdf/1607.06450.pdf">층 정규화 논문</a></li>
</ul>

<h4 id="encoder-layer-code"><span class="me-2">Encoder layer code</span><a href="#encoder-layer-code" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="k">def</span> <span class="nf">encoder_layer</span><span class="p">(</span><span class="n">dff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">encoder_layer</span><span class="sh">"</span><span class="p">):</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">inputs</span><span class="sh">"</span><span class="p">)</span>

  <span class="c1"># 인코더는 패딩 마스크 사용
</span>  <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">padding_mask</span><span class="sh">"</span><span class="p">)</span>

  <span class="c1"># 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)
</span>  <span class="n">attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span>
      <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">)({</span>
          <span class="sh">'</span><span class="s">query</span><span class="sh">'</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="sh">'</span><span class="s">key</span><span class="sh">'</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="c1"># Q = K = V
</span>          <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">padding_mask</span> <span class="c1"># 패딩 마스크 사용
</span>      <span class="p">})</span>

  <span class="c1"># 드롭아웃 + 잔차 연결과 층 정규화
</span>  <span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">attention</span><span class="p">)</span>
  <span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LayerNormalization</span><span class="p">(</span>
      <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">attention</span><span class="p">)</span>

  <span class="c1"># 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)
</span>  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">dff</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">attention</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">d_model</span><span class="p">)(</span><span class="n">outputs</span><span class="p">)</span>

  <span class="c1"># 드롭아웃 + 잔차 연결과 층 정규화
</span>  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">outputs</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LayerNormalization</span><span class="p">(</span>
      <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)(</span><span class="n">attention</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</code></div></div>

<h3 id="encoder-code"><span class="me-2">Encoder Code</span><a href="#encoder-code" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">encoder</span><span class="sh">"</span><span class="p">):</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">inputs</span><span class="sh">"</span><span class="p">)</span>

  <span class="c1"># 인코더는 패딩 마스크 사용
</span>  <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">padding_mask</span><span class="sh">"</span><span class="p">)</span>

  <span class="c1"># 포지셔널 인코딩 + 드롭아웃
</span>  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">embeddings</span> <span class="o">*=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
  <span class="n">embeddings</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)(</span><span class="n">embeddings</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">embeddings</span><span class="p">)</span>

  <span class="c1"># 인코더를 num_layers개 쌓기
</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">encoder_layer</span><span class="p">(</span><span class="n">dff</span><span class="o">=</span><span class="n">dff</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">encoder_layer_{}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
    <span class="p">)([</span><span class="n">outputs</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</code></div></div>

<hr />

<h2 id="3-decoder"><span class="me-2">3. Decoder</span><a href="#3-decoder" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<h3 id="first-sublayer--masked-multi-head-self-attention"><span class="me-2">First sublayer : Masked Multi-head Self-Attention</span><a href="#first-sublayer--masked-multi-head-self-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<ul>
  <li>기본적으로 Transformer에서는 Multi-head Attention을 수행하고, 내부에서 Scaled dot-product Attention Function을 호출하여 Masking을 적용
    <ol>
      <li><strong>Encoders</strong>’ Self-Attention : Padding Mask</li>
      <li><strong>Decoders</strong>’ Masked Self-Attention : Look-ahead Mask</li>
      <li><strong>Decoders</strong>’ Encoder-Decoder Attention : Padding Mask</li>
    </ol>
  </li>
</ul>

<h4 id="look-ahead-mask"><span class="me-2">Look-ahead Mask</span><a href="#look-ahead-mask" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<ul>
  <li>Input : Embedding layer + Positional Encoding(with Teacher Forcing)</li>
  <li>번역할 문장을 문장 행렬로 입력 받을 때, RNN과는 달리 <em>한꺼번에</em> 입력되어 현재 시점의 단어를 예측하는 데에 미래 시점의 단어까지 참고</li>
  <li>Look-ahead Mask : 현재 시점보다 미래에 있는 단어를 참고하지 못하도록 하는 역할</li>
  <li>Masked Self-Attention : Self-Attention을 통해 Attention score를 얻고, Scaled dot-product Attention Function에 Look-ahead mask를 전달 후 Masking을 실시; 자기 자신과 이전 단어들 만을 참고할 수 있음</li>
</ul>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="c1"># 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수 
</span><span class="k">def</span> <span class="nf">create_look_ahead_mask</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
	<span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
	<span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">band_part</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
	<span class="n">padding_mask</span> <span class="o">=</span> <span class="nf">create_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 패딩 마스크도 포함
</span>	<span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>
</code></div></div>

<h3 id="second-sublayer--encoder-decoder-attention"><span class="me-2">Second sublayer : Encoder-Decoder Attention</span><a href="#second-sublayer--encoder-decoder-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<h4 id="q--decoder-k--v--encoder"><span class="me-2">Q : Decoder, K = V : Encoder</span><a href="#q--decoder-k--v--encoder" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<ul>
  <li>Q, K, V의 출처가 같을 경우 Self-Attention이라고 함</li>
  <li>Query는 Decoder의 1st sublayer로부터, Key, Value는 Encoder의 마지막 layer로부터 받음</li>
</ul>

<h4 id="decode-layer-code"><span class="me-2">Decode layer code</span><a href="#decode-layer-code" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="k">def</span> <span class="nf">decoder_layer</span><span class="p">(</span><span class="n">dff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">decoder_layer</span><span class="sh">"</span><span class="p">):</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">inputs</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">enc_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">encoder_outputs</span><span class="sh">"</span><span class="p">)</span>

  <span class="c1"># 룩어헤드 마스크(첫번째 서브층)
</span>  <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">look_ahead_mask</span><span class="sh">"</span><span class="p">)</span>

  <span class="c1"># 패딩 마스크(두번째 서브층)
</span>  <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">padding_mask</span><span class="sh">'</span><span class="p">)</span>

  <span class="c1"># 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)
</span>  <span class="n">attention1</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span>
      <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">attention_1</span><span class="sh">"</span><span class="p">)(</span><span class="n">inputs</span><span class="o">=</span><span class="p">{</span>
          <span class="sh">'</span><span class="s">query</span><span class="sh">'</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="sh">'</span><span class="s">key</span><span class="sh">'</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="c1"># Q = K = V
</span>          <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">look_ahead_mask</span> <span class="c1"># 룩어헤드 마스크
</span>      <span class="p">})</span>

  <span class="c1"># 잔차 연결과 층 정규화
</span>  <span class="n">attention1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LayerNormalization</span><span class="p">(</span>
      <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)(</span><span class="n">attention1</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">)</span>

  <span class="c1"># 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)
</span>  <span class="n">attention2</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span>
      <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">attention_2</span><span class="sh">"</span><span class="p">)(</span><span class="n">inputs</span><span class="o">=</span><span class="p">{</span>
          <span class="sh">'</span><span class="s">query</span><span class="sh">'</span><span class="p">:</span> <span class="n">attention1</span><span class="p">,</span> <span class="sh">'</span><span class="s">key</span><span class="sh">'</span><span class="p">:</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">:</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="c1"># Q != K = V
</span>          <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">padding_mask</span> <span class="c1"># 패딩 마스크
</span>      <span class="p">})</span>

  <span class="c1"># 드롭아웃 + 잔차 연결과 층 정규화
</span>  <span class="n">attention2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">attention2</span><span class="p">)</span>
  <span class="n">attention2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LayerNormalization</span><span class="p">(</span>
      <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)(</span><span class="n">attention2</span> <span class="o">+</span> <span class="n">attention1</span><span class="p">)</span>

  <span class="c1"># 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)
</span>  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">dff</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">attention2</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">d_model</span><span class="p">)(</span><span class="n">outputs</span><span class="p">)</span>

  <span class="c1"># 드롭아웃 + 잔차 연결과 층 정규화
</span>  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">outputs</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LayerNormalization</span><span class="p">(</span>
      <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)(</span><span class="n">outputs</span> <span class="o">+</span> <span class="n">attention2</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">],</span>
      <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</code></div></div>

<h3 id="third-sublayer--ffnn"><span class="me-2">Third sublayer : FFNN</span><a href="#third-sublayer--ffnn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<h3 id="decoder-code"><span class="me-2">Decoder Code</span><a href="#decoder-code" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">decoder</span><span class="sh">'</span><span class="p">):</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">inputs</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">enc_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">encoder_outputs</span><span class="sh">'</span><span class="p">)</span>

  <span class="c1"># 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.
</span>  <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">look_ahead_mask</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">padding_mask</span><span class="sh">'</span><span class="p">)</span>

  <span class="c1"># 포지셔널 인코딩 + 드롭아웃
</span>  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">embeddings</span> <span class="o">*=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
  <span class="n">embeddings</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)(</span><span class="n">embeddings</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">embeddings</span><span class="p">)</span>

  <span class="c1"># 디코더를 num_layers개 쌓기
</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">decoder_layer</span><span class="p">(</span><span class="n">dff</span><span class="o">=</span><span class="n">dff</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">decoder_layer_{}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">outputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">],</span>
      <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</code></div></div>

<hr />

<h2 id="4-position-wise-ffnn"><span class="me-2">4. Position-wise FFNN</span><a href="#4-position-wise-ffnn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<ul>
  <li>Encoder, Decoder에서 공통적으로 가지고 있는 FFNN형태의 sublayer</li>
</ul>

<h2 id="5-transformer-implementation"><span class="me-2">5. Transformer Implementation</span><a href="#5-transformer-implementation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<h3 id="transformer-code"><span class="me-2">Transformer Code</span><a href="#transformer-code" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="k">def</span> <span class="nf">transformer</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
                <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">transformer</span><span class="sh">"</span><span class="p">):</span>

  <span class="c1"># 인코더의 입력
</span>  <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">inputs</span><span class="sh">"</span><span class="p">)</span>

  <span class="c1"># 디코더의 입력
</span>  <span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">dec_inputs</span><span class="sh">"</span><span class="p">)</span>

  <span class="c1"># 인코더의 패딩 마스크
</span>  <span class="n">enc_padding_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span>
      <span class="n">create_padding_mask</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span>
      <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">enc_padding_mask</span><span class="sh">'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>

  <span class="c1"># 디코더의 룩어헤드 마스크(첫번째 서브층)
</span>  <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span>
      <span class="n">create_look_ahead_mask</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span>
      <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">look_ahead_mask</span><span class="sh">'</span><span class="p">)(</span><span class="n">dec_inputs</span><span class="p">)</span>

  <span class="c1"># 디코더의 패딩 마스크(두번째 서브층)
</span>  <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span>
      <span class="n">create_padding_mask</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span>
      <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">dec_padding_mask</span><span class="sh">'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>

  <span class="c1"># 인코더의 출력은 enc_outputs. 디코더로 전달된다.
</span>  <span class="n">enc_outputs</span> <span class="o">=</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dff</span><span class="o">=</span><span class="n">dff</span><span class="p">,</span>
      <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
  <span class="p">)(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">])</span> <span class="c1"># 인코더의 입력은 입력 문장과 패딩 마스크
</span>
  <span class="c1"># 디코더의 출력은 dec_outputs. 출력층으로 전달된다.
</span>  <span class="n">dec_outputs</span> <span class="o">=</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dff</span><span class="o">=</span><span class="n">dff</span><span class="p">,</span>
      <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
  <span class="p">)(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">])</span>

  <span class="c1"># 다음 단어 예측을 위한 출력층
</span>  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">outputs</span><span class="sh">"</span><span class="p">)(</span><span class="n">dec_outputs</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</code></div></div>

<h3 id="hyper-parameter-setting"><span class="me-2">Hyper-parameter Setting</span><a href="#hyper-parameter-setting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="n">small_transformer</span> <span class="o">=</span> <span class="nf">transformer</span><span class="p">(</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">9000</span><span class="p">,</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">dff</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">small_transformer</span><span class="sh">"</span><span class="p">)</span>

<span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">plot_model</span><span class="p">(</span>
    <span class="n">small_transformer</span><span class="p">,</span> <span class="n">to_file</span><span class="o">=</span><span class="sh">'</span><span class="s">small_transformer.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></div></div>

<h3 id="loss-function"><span class="me-2">Loss Function</span><a href="#loss-function" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
  <span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">MAX_LENGTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nc">SparseCategoricalCrossentropy</span><span class="p">(</span>
      <span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">)(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">not_equal</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</code></div></div>

<h3 id="learning-rate"><span class="me-2">Learning rate</span><a href="#learning-rate" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<h4 id="learning-rate-scheduler"><span class="me-2">Learning rate Scheduler</span><a href="#learning-rate-scheduler" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<ul>
  <li>미리 학습 일정을 정해두고 그 일정에 따라 학습률이 조정되는 방법</li>
  <li>Transformer의 경우, 사용자가 정한 단계까지는 학습률을 증가시키고, 일정 단계에 이르면 학습률을 점차적으로 하강하여 적용</li>
</ul>

<h4 id="cumstom-scheduler-code"><span class="me-2">Cumstom Scheduler Code</span><a href="#cumstom-scheduler-code" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><span class="k">class</span> <span class="nc">CustomSchedule</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">schedules</span><span class="p">.</span><span class="n">LearningRateSchedule</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">4000</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">(</span><span class="n">CustomSchedule</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">arg1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">rsqrt</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
    <span class="n">arg2</span> <span class="o">=</span> <span class="n">step</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">warmup_steps</span><span class="o">**-</span><span class="mf">1.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">rsqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">minimum</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</code></div></div>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/nlp/">NLP</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/concept/"
            class="post-tag no-text-decoration"
          >Concept</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=Transformer%20Algorithm%20-%20Patrick's%20Dev.%20Blog&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FTransformer%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=Transformer%20Algorithm%20-%20Patrick's%20Dev.%20Blog&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FTransformer%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FTransformer%2F&text=Transformer%20Algorithm%20-%20Patrick's%20Dev.%20Blog" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Graph/">Graph</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/pip_requiremntes/">Pip_requiremntes</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/BPE/">Bpe</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Huggingface/">Huggingface</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Datasets/">Datasets</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/concept/">Concept</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/grammer/">Grammer</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/bfs/">BFS</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/dfs/">DFS</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/graph/">Graph</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/paper/">Paper</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">Tutorial</a>
      
    </div>
  </section>


            </div>

            
              
              






  <div class="toc-border-cover z-3"></div>
  <section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4">
    <h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    










  <aside id="related-posts" aria-labelledby="related-label">
    <h3 class="mb-4" id="related-label">Further Reading</h3>
    <nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        <article class="col">
          <a href="/posts/Attention_Machanism/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1701774720"
  data-df="ll"
  
>
  Dec  5, 2023
</time>

              <h4 class="pt-0 my-2">Attention Algorithm</h4>
              <div class="text-muted">
                <p>Seq2seq based on RNN seq2seq Model : RNN에 기반하여 Encoder에서 입력 시퀀스를 context vector라는 하나의 고정된 크기의 벡터 표현으로 압축하고, Decoder에서 출력 시퀀스를 생성 문제점    하나의 고정된 크기의 벡터에 모든 정보를 압축; 정보 손실이 발생   RNN의 문제점 중 하나인 Vanishi...</p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/Subword_Tokenizer/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1701168600"
  data-df="ll"
  
>
  Nov 28, 2023
</time>

              <h4 class="pt-0 my-2">Subword Tokenizer</h4>
              <div class="text-muted">
                <p>Subword Tokenizer Subword    OOV(Out-Of Vocabulary) : 새로 제시된 단어에 대해 기존의 dictionary 내의 단어가 아닌 경우   Subword : 하나의 단어를 구성하는 더 작은 단위의 의미를 가진 어구   Subword segmenation : 하나의 단어를 여러 서브 워드로 분리해서 단어를 인코딩 및 ...</p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/LSTM/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1700556180"
  data-df="ll"
  
>
  Nov 21, 2023
</time>

              <h4 class="pt-0 my-2">Long Short-Term Memory in RNN</h4>
              <div class="text-muted">
                <p>Long Short-Term Memory; LSTM The problem of long-term dependencies    Vanila RNN(기본형 RNN)은 비교적 짧은 시퀀스에 대해서만 효과를 보임(시점이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상 발생)   LSTM    은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 ...</p>
              </div>
            </div>
          </a>
        </article>
      
    </nav>
  </aside>
  <!-- #related-posts -->


            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/posts/Attention_Machanism/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>Attention Algorithm</p>
    </a>
  

  
    <a
      href="/posts/Obsidian/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>Obsidian</p>
    </a>
  
</nav>

            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2025</time>

    
      <a href="https://twitter.com/username">Patrick</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="v7.2.4"
        href="https://github.com/cotes2020/jekyll-theme-chirpy"
        target="_blank"
        rel="noopener"
      >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/concept/">Concept</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/grammer/">Grammer</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/bfs/">BFS</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/dfs/">DFS</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/graph/">Graph</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/paper/">Paper</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">Tutorial</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- Embedded scripts -->

    
      
      <!-- The comments switcher -->


    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  
  document.addEventListener('DOMContentLoaded', () => {
    SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('search-results'),
      json: '/assets/js/data/search.json',
      searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{content}</p>  </article>',
      noResultsText: '<p class="mt-5">Oops! No results found.</p>',
      templateMiddleware: function(prop, value, template) {
        if (prop === 'categories') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
          }
        }

        if (prop === 'tags') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
          }
        }
      }
    });
  });
</script>

  </body>
</html>

