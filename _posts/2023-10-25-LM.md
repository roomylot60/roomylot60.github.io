---
title: Language Model & Modeling
author: Patrick
date: 2023-10-25 18:17:00 +0900
tags: [Concept]
categories: [NLP]
render_with_liquid: false
---

## Language Model(언어 모델)
- **Language Model** : 단어 sequence(문장)에 문장의 적절성에 대한 확률(단어에 대한 예측을 통해)을 할당(assign)하는 모델
	1. Statistical(통계적) 방법
	2. Artifitial Neural Network(인공 신경망)을 사용한 방법
- Language Modeling : 주어진 단어들로 부터 아직 모르는 단어를 예측하는 작업

### 단어 예측
1. 단어 sequence의 확률 : 하나의 단어를 `w`, 단어 sequence를 `W`라고 한다면, `n`개의 단어가 등장하는 단어 sequence `W`의 확률은<br>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/n_probability.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

2. 다음 단어 등장 확률 : `n-1`개의 단어가 나열된 상태에서 `n`번째 단어의 확률은 *조건부 확률*
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/n_conditional_probability.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div><br>
전체 단어 sequence `W`의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 <br>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/LM_conditional_probability.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Statistical Language Model, SLM
#### Chain Rule
- 문장(단어 sequence)의 확률은 문맥이라는 관계로 인해 이전 단어의 영향을 받아 다음 단어가 등장
- 조건부 확률을 통해 일반화하면 이전 단어에 대한 등장 확률의 곱으로 나타남

#### Count Based Approach
- 기계가 학습한 corpus data에서 특정 단어 다음에 등장한 단어의 횟수를 확률로 사용
- LM은 실생활에서 사용되는 언어의 확률 분포를 주어진 Corpus data를 이용한 학습을 통해 근사 모델링 실시
- **Sparsity problem(희소 문제)** : 충분한 데이터를 관측하지 못하여 정확한 모델링을 할 수 없는 희소 문제가 발생할 수 있어, *n-gram LM*이나 여러 *generalization(일반화)* 기법(ex - smoothing, back-off)을 사용해 이를 보충

#### N-gram LM
- 전체 단어에 대해 고려하는 일반 LM과는 달리 *`n`개의 단어에 대한 확률*을 사용
- 문장이 길수록 corpus 내에서 동일한 문장이 존재하지 않을 확률이 증가하므로, 참고하는 단어의 수를 줄임으로써 이를 근사치로 활용
- 참고 단어의 수에 따라 의도와는 다른 결과를 예측하거나, n이 클수록 모델 사이즈가 커지는 문제가 발생

#### Perplexity; PPL
- LM의 성능 비교에서 time cost를 줄이기 위해 사용하는 평가 지표로,  Branching Factor(선택할 수 있는 가능한 경우의 수)를 의미
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/PPL.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

---

## Count based word Representation
- 텍스트를 수치화하여 통계적으로 접근
- 텍스트 데이터 내에서 특정 단어의 중요도, 문서의 핵심어 추출, 검색 엔진에서 검색 결과의 순위 결정, 문서들 간의 유사도를 구하는 등의 용도로 사용

### Word Representation(단어의 표현 방법)
- Local Representation(=Discrete Representation)(국소 표현) : 해당 단어 그 자체만 보고, 특정값을 매핑하여 단어를 표현
- Distributed Representation(=Continuous Representation)(분산 표현) : 특정 단어를 표현하고자 주변을 참고하여 표현
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/wordrepresentation.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### **BoW(Bag of Words)**
- Bag of Words : 단어들의 출현 빈도(frequency)만을 고려하여 수치화, 순서는 고려하지 않음

```python
from konlpy.tag import Okt

okt = Okt()

def build_bag_of_words(document):
    # 온점 제거 및 형태소 분석
    document = document.replace('.', '')
    tokenized_document = okt.morphs(document)

    word_to_index = {}
    bow = []

    for word in tokenized_document:  
    if word not in word_to_index.keys():
        word_to_index[word] = len(word_to_index)  
        # BoW에 전부 기본값 1을 넣는다.
        bow.insert(len(word_to_index) - 1, 1)
    else:
        # 재등장하는 단어의 인덱스
        index = word_to_index.get(word)
        # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.
        bow[index] = bow[index] + 1

    return word_to_index, bow
    
# CountVenctorizer in Scikit-learn pakage
# 영어 기준이기 때문에 띄어쓰기에 따라 토큰화 -> 한국어에서 조사 등의 구분이 되지 않음
from sklearn.feature_extraction.text import CountVectorizer

corpus = ['you know I want your love. because I love you.']
vector = CountVectorizer()

# 코퍼스로부터 각 단어의 빈도수를 기록
print('bag of words vector :', vector.fit_transform(corpus).toarray()) 

# 각 단어의 인덱스가 어떻게 부여되었는지를 출력
print('vocabulary :',vector.vocabulary_)
```

### DTM; Document Term Matrix(문서 단어 행렬)
- 다수의 문서에 대해 각 문서의 BoW를 합쳐서 하나의 행렬로 표현
- 전체 corpus의 크기가 클수록 Sparse representation에 대한 손실(0인 값을 갖는 벡터가 많아짐)이 발생 : 구두점, 낮은 빈도수의 단어, 불용어의 제거 혹은 표제어, 어간 추출을 통해 정규화를 진행하여 보완
- 단순히 빈도수가 높은 단어가 중요도가 높다는 근거 부족(불용어)

### TF-IDF; Term Frequency-Inverse Document Frequency
- DTM 내의 각 단어들마다 중요도에 따른 가중치 부여
- 문서를 d, 단어를 t, 문서의 총 개수를 n이라고 표현할 때,
    * `tf(d,t)` : 특정 문서 d에서의 특정 단어 t의 등장 횟수(tf; token frequency)
    * `df(t)` : 특정 단어 t가 등장한 문서의 수(df; document frequency)
    * `idf(t)` : `df(t)`에 반비례하는 수(idf; inverse df)<br>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/idf.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단
- 문서의 유사도, 검색 시스템에서 검색 결과의 중요도 부여, 문서 내의 특정 단어의 중요도 측정 등에 사용

```python
import pandas as pd # 데이터프레임 사용을 위해
from math import log # IDF 계산을 위해

docs = [
    '먹고 싶은 사과',
    '먹고 싶은 바나나',
    '길고 노란 바나나 바나나',
    '저는 과일이 좋아요'
] 
vocab = list(set(w for doc in docs for w in doc.split()))
vocab.sort()

# 총 문서의 수
N = len(docs) 

def tf(t, d):
    return d.count(t)

def idf(t):
    df = 0
    for doc in docs:
        df += t in doc
    return log(N/(df+1))

def tfidf(t, d):
    return tf(t,d)* idf(t)

result = []
# 각 문서에 대해서 아래 연산을 반복
for i in range(N):
    result.append([])
    d = docs[i]
    for j in range(len(vocab)):
        t = vocab[j]
        result[-1].append(tf(t, d))

tf_ = pd.DataFrame(result, columns = vocab)

result = []
for j in range(len(vocab)):
    t = vocab[j]
    result.append(idf(t))

idf_ = pd.DataFrame(result, index=vocab, columns=["IDF"])
idf_

result = []
for i in range(N):
    result.append([])
    d = docs[i]
    for j in range(len(vocab)):
        t = vocab[j]
        result[-1].append(tfidf(t,d))

tfidf_ = pd.DataFrame(result, columns = vocab)
tfidf_
```

---

## Vector Similarity
- 문장이나 문서의 유사도를 구하는 작업
    1. 어떤 방법으로 수치화하여 표현했는가
    2. 문서 간의 단어들의 차이를 어떤 방법으로 계산했는가
### Cosine Similarity
- BoW 기반 표현방법(DTM, TF-IDF), Word2Vec 등을 통해 문장을 벡터화
- 두 벡터 간의 cosine 각도를 이용하여 두 벡터의 유사도를 측정<br>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/cosine_similarity.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

```python
import numpy as np
from numpy import dot
from numpy.linalg import norm

def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

doc1 = np.array([0,1,1,1])
doc2 = np.array([1,0,1,1])
doc3 = np.array([2,0,2,2])

print('문서 1과 문서2의 유사도 :',cos_sim(doc1, doc2))
print('문서 1과 문서3의 유사도 :',cos_sim(doc1, doc3))
print('문서 2와 문서3의 유사도 :',cos_sim(doc2, doc3))
```
### Euclidean Distance
- 공간에 위치한 두 점 p, q사이의 직선거리<br>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/euclidean_distance.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

```python
import numpy as np

def dist(x,y):   
    return np.sqrt(np.sum((x-y)**2))

doc1 = np.array((2,3,0,1))
doc2 = np.array((1,2,3,1))
doc3 = np.array((2,1,2,2))
docQ = np.array((1,1,0,1))

print('문서1과 문서Q의 거리 :',dist(doc1,docQ))
print('문서2과 문서Q의 거리 :',dist(doc2,docQ))
print('문서3과 문서Q의 거리 :',dist(doc3,docQ))
```

### Jaccard Similarity
- 교집합을 갖는 서로 다른 집합 A, B에서 합집합에 대한 교집합의 비율
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/jaccard_similarity.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## [Machine Learning](./Machine%20Learning.md)
## [Deep Learning](./Deep%20Learning)