---
title: 
author: Patrick
date: 2024-01-19 13:18:00 +0900
tags: []
categories: []
render_with_liquid: false
---
## Visual Geometric Group; VGG
### Basic VGGnet
![VGG-16-Architecture](../Attatched/Pasted%20image%2020240119163040.png)
- Input Layer : `224x224xRGB(3)` images
- 13 CNN Layers(2, 2, 3, 3)
	- Kernel : `3x3` filter
	- Stride : `1` pixel
	- Padding : `1` pixel for `3x33x3`
	- Pooling : Max Pooling `2x2`, stide fixed to `2`
- 3 Fully conenected Layers : `4096` channels, `4096` channels, Softmax layer with `1000` channels

### Local Response Normalizaton; LRN
- 일반적으로는 메모리 소비, 훈련 시간 증가로 인해 사용하지 않음
- 정확도의 향상과 관계가 없음
- ReLU 사용 시, 양수 방향으로 입력값을 그대로 사용
- Convolution, Pooling 에서 매우 높은 수치의 픽셀 값이 주변 픽셀에 영향을 주는데, 이를 억제하기 위한 용도로 사용(why?)

### Dilated Convolution
![Dilated_Conv](../Attatched/Pasted%20image%2020240119165019.png)
- Dilation Rate : Kernel 사이의 간격을 지정하여 동일한 비용으로 더 넓은 시야를 제공
- Real-time segmentation 분야에서 주로 사용하고, 여러 convolution이나 큰 kernel을 사용할 여유가 없을 경우에 사용
- 필터 내부에 zero padding 을 추가하여, 강제로 receptive field를 늘리는 방법
	- Receptive field : 필터가 한 번 보는 영역으로, 사진의 feature 를 추출할 때 높을 수록 성능이 좋음
### Transposed Convolution

### Ref.
[Convolution](https://zzsza.github.io/data/2018/02/23/introduction-convolution/)
[VGG](https://wikidocs.net/164796)
![VGG](../Attatched/Pasted%20image%2020240125173611.png)

### Why not more CNN layers in VGGnet?
- CNN 레이어의 수가 증가할 수록, 더 복잡한 기능에 맞는 모델의 성능도 증가
- 신경망의 가중치는 역전파 과정을 통해 업데이트 되는데, 각 가중치를 변경하여 모델의 손실을 줄임
- 손실이 감소하는 방향으로 단계를 수행하는 중 Gradient vanishing이 발생하여 훈련 시간을 증가 시킴
- Residual Network; ResNet 같은 방식으로 이 문제를 해결

## ResNet
