---
title: MoeGoe TTS Code Review
author: Patrick
date: 2025-03-26 20:50:00 +0900
tags: [VITS]
categories: [Algorithm, Concept]
render_with_liquid: false
---
## MoeGoe Project
### 구성
MoeGoe/
attentions.py
modules.py

### attentions.py
#### 1. Encoder 클래스
- 역할: 입력 시퀀스를 받아 여러 층의 셀프 어텐션과 피드포워드 네트워크를 통해 특징을 추출​
- 구성 요소:
    - `attn_layers`: 입력 내용과 관련된 멀티헤드 어텐션 층의 리스트
    - `norm_layers_1`: 첫 번째 정규화 층의 리스트로, 어텐션 후에 적용
    - `ffn_layers`: 피드포워드 네트워크 층의 리스트로, 비선형 변환을 적용
    - `norm_layers_2`: 두 번째 정규화 층의 리스트로, 피드포워드 네트워크 후에 적용​
- 순전파 과정:
    - 입력 x에 마스크 `x_mask`를 적용하여 불필요한 부분을 제거​
    - 각 층에 대해:
        멀티헤드 어텐션을 수행하고 드롭아웃과 정규화를 수행
        피드포워드 네트워크를 통과한 후 드롭아웃과 정규화를 수행
        최종 출력을 반환

#### 2. Decoder 클래스
- 역할: 인코더의 출력을 받아 목표 시퀀스를 생성
- 구성 요소:
    - `self_attn_layers`: 디코더의 셀프 어텐션 층으로, 이전 출력에 대한 정보를 활용
    - `norm_layers_0`: 셀프 어텐션 후의 정규화 층
    - `encdec_attn_layers`: 인코더-디코더 어텐션 층으로, 인코더의 출력을 참고
    - `norm_layers_1`: 인코더-디코더 어텐션 후의 정규화 층
    - `ffn_layers`: 피드포워드 네트워크 층으로, 비선형 변환을 수행
    - `norm_layers_2`: 피드포워드 네트워크 후의 정규화 층
- 순전파 과정:
    - 입력 x와 인코더 출력 h에 마스크를 적용
    - 각 층에 대해:​
        셀프 어텐션을 수행하고 드롭아웃과 정규화를 수행
        인코더-디코더 어텐션을 수행하고 드롭아웃과 정규화를 수행
        피드포워드 네트워크를 통과한 후 드롭아웃과 정규화를 수행
    최종 출력을 반환

#### 3. MultiHeadAttention 클래스
- 역할: 입력에 대해 여러 개의 어텐션 헤드를 사용하여 다양한 표현 공간에서 정보를 추출
- 구성 요소:
    - `conv_q`, `conv_k`, `conv_v`: 쿼리, 키, 밸류를 생성하는 1D 컨볼루션 층
    - `conv_o`: 최종 출력을 생성하는 1D 컨볼루션 층
    - `drop`: 드롭아웃 층으로, 과적합을 방지
    - `emb_rel_k`, `emb_rel_v`: 상대적 위치 임베딩을 위한 매개변수로, 위치 정보를 어텐션에 활용
- 순전파 과정:
    - 입력 x와 컨텍스트 c로부터 쿼리, 키, 밸류를 생성
    - 어텐션 스코어를 계산하고, 필요 시 상대적 위치 정보를 추가
    - 소프트맥스를 통해 어텐션 가중치를 얻고, 이를 밸류에 적용하여 출력을 계산하여 최종 출력을 반환

#### 4. FFN 클래스
- 역할: 각 위치별로 독립적인 피드포워드 네트워크를 적용하여 비선형 변환을 수행
- 구성 요소:
    - `conv_1`: 입력을 확장된 차원으로 변환하는 1D 컨볼루션 층
    - `conv_2`: 확장된 차원을 다시 원래 차원으로 축소하는 1D 컨볼루션 층
    - `drop`: 드롭아웃 층으로, 과적합을 방지
- 순전파 과정:
    - 입력에 패딩을 적용한 후 첫 번째 컨볼루션과 활성화 함수를 적용
    - 드롭아웃을 적용한 후 두 번째 컨볼루션을 적용
    - 출력에 마스크를 적용하여 최종 출력을 도출

#### Multi-Attention 변형 방식 정리
- 상대적 위치 임베딩 (Relative Position Embeddings): 
    - window_size 매개변수를 통해 상대적 위치 정보를 활용하여 어텐션 점수를 조정
    - 이는 문장 내 단어들의 상대적 위치 정보를 반영하여 모델의 성능을 향상시키는 역할을 담당
- 헤드 간 파라미터 공유 (Head-wise Parameter Sharing):
    - heads_share 매개변수를 통해 모든 어텐션 헤드가 동일한 상대적 위치 임베딩을 공유할지 여부를 결정
    - 모델의 복잡도를 조절
- 블록 길이 제한 (Block Length Restriction):
    - block_length 매개변수를 사용하여 로컬 어텐션을 수행할 때, 각 토큰이 주의할 수 있는 최대 토큰 수를 제한
    - 긴 시퀀스 처리 시 계산 효율성을 향상
- 근접 바이어스 (Proximal Bias):
    - proximal_bias 매개변수가 활성화되면, 인접한 위치에 더 높은 어텐션 점수를 부여하도록 설계
    - 이는 가까운 위치의 토큰들 간의 상호작용을 강조하는 데 사용
- 근접 초기화 (Proximal Initialization):
    - proximal_init 매개변수가 True로 설정되면, 쿼리와 키의 가중치가 동일하게 초기화
    - 이는 학습 초기 단계에서 쿼리와 키의 유사성을 높여 안정적으로 학습

---

### modules.py
#### 1. LayerNorm 클래스
- 기능: 입력 텐서의 마지막 차원에 대해 레이어 정규화를 적용하여 학습 안정성과 속도를 향상​
- 구성 요소:
    - `gamma`: 정규화된 출력에 대한 스케일 파라미터 (학습 가능)​
    - `beta`: 정규화된 출력에 대한 이동 파라미터 (학습 가능)​
- 순전파 과정:
    - 입력 텐서를 마지막 차원으로 전치(transpose)
    - F.layer_norm을 사용하여 정규화
    - 원래 차원 순서로 환원

#### 2. ConvReluNorm 클래스
- 기능: 1D 컨볼루션, ReLU 활성화 함수, 드롭아웃, 그리고 레이어 정규화를 결합하여 입력 데이터를 변환
- 구성 요소:
    - `conv_layers`: 1D 컨볼루션 레이어들의 리스트​
    - `norm_layers`: 각 컨볼루션 레이어에 대응하는 레이어 정규화 모듈들의 리스트​
    - `relu_drop`: ReLU 활성화 함수와 드롭아웃을 포함하는 시퀀스​
    - `proj`: 출력을 원하는 채널 수로 투영하는 1D 컨볼루션 레이어​
- 순전파 과정:
    - 입력에 대해 각 컨볼루션과 정규화를 수행한 후, ReLU와 드롭아웃을 적용​
    - 최종적으로 원본 입력에 투영된 출력을 더하여 반환

#### 3. DDSConv 클래스(Dilated and Depth-Separable Convolution)
- 기능: 팽창된 심층 분리 컨볼루션(Dilated and Depth-Separable Convolution)을 구현하여, 수용 영역을 확장하면서도 파라미터 수를 효율적으로 유지
- 구성 요소:
    - `convs_sep`: 심층 분리 컨볼루션 레이어들의 리스트​
    - `convs_1x1`: 1x1 컨볼루션 레이어들의 리스트​
    - `norms_1`, `norms_2`: 각 컨볼루션 후에 적용되는 레이어 정규화 모듈들의 리스트​
    - `drop`: 드롭아웃 모듈​
- 순전파 과정:
    - 각 컨볼루션과 정규화, 활성화 함수를 순차적으로 적용
    - 드롭아웃을 적용한 후, 원본 입력에 결과를 더하여 반환

#### 4. WN 클래스
- 기능: WaveNet 스타일의 컨디셔널 컨볼루션 블록을 구현하여, 음성 합성 모델에서 사용
- 구성 요소:
    - `in_layers`: 입력에 대한 컨볼루션 레이어들의 리스트​
    - `res_skip_layers`: 잔차(residual) 및 스킵(skip) 연결을 위한 레이어들의 리스트​
    - `cond_layer`: 조건부 입력(g) 처리를 위한 컨볼루션 레이어 (옵션)​
    - `drop`: 드롭아웃 모듈​
- 순전파 과정:
    - 입력에 대해 각 컨볼루션을 수행하고, 조건부 입력이 있을 경우 이를 추가로 처리
    - 활성화 함수와 드롭아웃을 적용한 후, 잔차 및 스킵 출력을 계산하여 반환​

#### 5. ResBlock1 및 ResBlock2 클래스
- 기능: 잔차 블록을 구현하여, 다양한 팽창률(dilation rate)을 가진 컨볼루션을 통해 수용 영역을 확장
- 구성 요소:
    - `convs1` (ResBlock1): 다양한 팽창률을 가진 첫 번째 컨볼루션 레이어들의 리스트​
    - `convs2` (ResBlock1): 팽창률이 1인 두 번째 컨볼루션 레이어들의 리스트​
    - `convs` (ResBlock2): 다양한 팽창률을 가진 컨볼루션 레이어들의 리스트​
- 순전파 과정:
    - 입력에 대해 각 컨볼루션과 활성화 함수를 순차적으로 적용
    - 원본 입력에 결과를 더하여 반환

#### 6. Log 클래스
- 기능: 입력 텐서에 로그 변환을 적용하거나, 역변환을 수행
- 순전파 과정: 로그 변환을 적용하고, 로그 결정식 값을 계산하여 반환

#### 7. Flip 클래스
- 기능: 입력 텐서의 채널 차원을 반전
- 순전파 과정: 채널 차원을 뒤집고, 로그 결정식 값을 반환

#### 8. ElementwiseAffine 클래스
- 기능: 입력 텐서에 채널에 대해 학습 가능한 매개변수인 'm'과 'logs'를 적용하여 선형 변환을 수행
- 순전파 과정:
    - 입력 텐서 x는 x0와 x1 두 개의 부분으로 분할
    - x0에 기반하여 x1에 선형 변환이 적용, m은 변환의 이동(shift)을, 스케일 값은 logs 형태로 저장되어 지수 함수(torch.exp)를 통해 실제 스케일 값으로 변환
    - 변환된 x1과 원래의 x0를 결합하여 최종 출력을 생성