---
title: 
author: Patrick
date: 2024-01-26 13:18:00 +0900
tags: [Paper]
categories: [OCR]
render_with_liquid: false
---
## Paper
### An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition
- 이미지 기반 sequence 인식을 위한 end-to-end 학습 가능한 신경망과 장면 텍스트 인식으로의 응용
#### End-to-end training
- 주어진 작업의 전체적인 과정을 하나의 모델로 직접 학습하는 접근 방식
- 입력부터 출력까지의 모든 단계를 하나의 통합된 모델에서 학습하는 것
- 여러 단계의 처리나 모듈을 갖는 대신에 단일 모델이 데이터로부터 특징을 추출하고, 목표 출력을 생성하는 방식으로 작동
- 복잡한 작업을 단순화하고 모델의 성능을 향상
- 중간 단계의 특징 추출(Feature extraction)이나 전처리에 대한 사전 지식이 제한된 상황에서 효과적

#### Abstract
- Image-based sequence recognition 중에서 **Scene Text recognition(장면 텍스트 인식 문제)** 에 대해 다룸
- Feature extraction, Sequence modeling, Transcription 과정을 통합한 Neural Network Architecture의 등장 
	1. 대부분의 기존 알고리즘이 module별로 각각 학습되고 조정되는 반면, 제안된 아키텍처는 end-to-end로 학습 가능
	2. Charactre segmentation이나 Horizontal scale normalization 없이, 임의 길이의 시퀀스를 처리
	3. predefined lexicon(미리 정의된 사전; dictionary)에 제한되지 않으며 사전 존재 여부에 관계없이 장면 텍스트 인식 작업에서 뛰어난 성능
	4. 현실에 적용하기에 더 유용하고 효율적이면서도 작은 모델을 생성
- IIIT-5K, Street View Text 및 ICDAR dataset을 포함한 표준 벤치마크에서 이전 모델들에 비해 성능 향상
- 이미지 기반 악보 인식 작업에서도 우수한 성능


#### Introduction
- 최근의 DCNN에 대한 연구는 객체 감지와 분류에 집중되어 있음
- 현실에서의 시각물; 장면 텍스트, 필적, 악보 등은 시각적 객체들의 나열인 sequence 형태이고, 이를 다루는 것이 computer vision에서의 주요 쟁점
- 일반적인 객체 인식에서 요구되는 수준은 단일(single)이 아닌 나열(sequence)에 대한 예측
- DCNN은 일정한 차원의 입력 및 출력에서 작동하며, 따라서 가변 길이의 레이블 sequence를 생성할 수 없기 때문 이러한 sequence 예측으로의 즉각적인 적용이 어려움
- 개별 문자를 탐지하고 이를 DCNN model 로 인식시켜 활용 : 강력한 문자 인식기의 필요
- 장면 텍스트 인식을 이미지 분류 문제로 다루어 클래스를 분류 : 지나치게 많은 클래스가 생성되어 일반화의 어려움을 가짐
- RNN에서는 이미지 내 각 요소의 위치 정보를 필요로 하지 않으나, 이미지 특성을 sequence로 변환하는 전처리의 필요한데, 이는 독립적인 요소로 end-to-end 훈련이 어려움 : 문자 이미지에서 기하학적 혹은 이미지적 특성을 Histogram Oriented Gradient; HOG 기반으로 연속적인 벡터화하여 추출


## torch.nn.Conv2d
```python
class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
```

- in_channels, out_channels : 입, 출력 채널의 크기
- kernel_size : Convolution에서 사용하는 kernel의 크기
- stride : kenel의 1회 이동 시 pixel 수
- padding : 입력에 적용할 padding의 양을 제어하는 parameter로, 정수값 / 정수로 된 tuple값으로 양변에 적용할 padding을 제공하며 `'valid', 'same'`과 같이 문자 입력도 가능
- dilation : kernel point 간의 간격을 제어
- groups : 입력과 출력간의 연결을 제어하며, 두 채널간의 block된 connection의 수; default=1
- padding_mode : padding 종류(ex - 'zeros', 'reflect' 'replicate' 'circular'); default='zeros'

```python
def __init__(self, input_channel, output_channel=512):
	super(VGG_FeatureExtractor, self).__init__()
	self.output_channel = [int(output_channel / 8), int(output_channel / 4),
						   int(output_channel / 2), output_channel]  # [64, 128, 256, 512]
	self.ConvNet = nn.Sequential(
		nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True), 
		nn.MaxPool2d(2, 2),  # 64x16x50
		nn.Conv2d(self.output_channel[0], self.output_channel[1], 3, 1, 1), nn.ReLU(True),
		nn.MaxPool2d(2, 2),  # 128x8x25
		nn.Conv2d(self.output_channel[1], self.output_channel[2], 3, 1, 1), nn.ReLU(True),  # 256x8x25
		nn.Conv2d(self.output_channel[2], self.output_channel[2], 3, 1, 1), nn.ReLU(True),
		nn.MaxPool2d((2, 1), (2, 1)),  # 256x4x25
		nn.Conv2d(self.output_channel[2], self.output_channel[3], 3, 1, 1, bias=False),
		nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),  # 512x4x25
		nn.Conv2d(self.output_channel[3], self.output_channel[3], 3, 1, 1, bias=False),
		nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),
		nn.MaxPool2d((2, 1), (2, 1)),  # 512x2x25
		nn.Conv2d(self.output_channel[3], self.output_channel[3], 2, 1, 0), nn.ReLU(True))  # 512x1x24
```

- 2D Convolution Layers, Max Pooling, Batch Normalization(Local Response Normalization)
- keras와 torch에서의 차이가 있음
- nn.Conv2d(input_channel, output_channel, kennel_size=(3,3), stride=1, padding=1; 'same')