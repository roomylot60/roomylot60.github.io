---
title: Long Short-Term Memory in RNN
author: Patrick
date: 2023-11-21 17:43:00 +0900
tags: [Concept]
categories: [NLP]
render_with_liquid: false
---
## Long Short-Term Memory; LSTM
### The problem of long-term dependencies
- Vanila RNN(기본형 RNN)은 비교적 짧은 시퀀스에 대해서만 효과를 보임(시점이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상 발생)

### LSTM
- 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억은 지우고, 기억해야할 것들을 선정
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/LSTM_architecture.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Cell state : 이전 시점의 셀 상태가 다음 시점의 셀 상태를 구하기 위한 입력으로 사용, 삭제 게이트의 값이 0에 가까울 수록 이전 시점의 셀 상태값의 영향력이 작아지고, 입력 게이트의 값이 현 시점의 셀 상태에 영향을 미침(`$$C_{t} = f_{t}ｏC_{t-1} + i_{t}ｏg_{t}$$`) 
    + Entrywise product : 두 행렬에서 같은 위치의 성분끼리의 곱을 통해 얻어지는 행렬
- 입력 게이트 : 현재 정보를 기억하기 위한 게이트(`i_t = sigmoid(x_t*W_(xi) + h_(t-1)*W_(hi)), g_t = tanh(x_t*W_(xg)+h_(t-1)*W_(hg))`)
- 삭제 게이트(망각 게이트) : 이전 시점의 입력을 얼마나 반영할 지를 결정, 기억을 삭제하기 위한 게이트로 0에 가까울 수록 많이 제거된 상태(`f_t = sigmoid(x_t*W_(xf) + h_(t-1)*W_(hf))`)
- 출력 게이트 : 현재 시점의 x값과 이전 시점의 은닉 상태가 시그모이드 함수를 지난 값으로 현재 시점의 은닉 상태를 결정(`o_t = sigmoid(x_t*W_(xo)+h_(t-1)*W_(ho)), h_t = o_tㅇtanh(c_t)`)

### Gated Recurrent Unit; GRU
- GRU : LSTM에서 3개의 게이트(출력, 입력, 삭제)를 사용했던 반면, GRU에서는 업데이트 게이트, 리셋 게이트 2개를 사용하여 LSTM의 구조를 간략화 
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/GRU.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### RNN Language Model
- Teacher Forcing(교사 강요) : 테스트 과정에서 t시점의 출력값이 t+1시점의 입력값으로 들어가도록 하는 RNN model에서, 훈련 과정 중에는 입력에 대한 예측값을 입력으로 하지 않고, 이미 정답을 알고 있는 레이블을 기반으로 훈련하여 훈련 과정을 단축하는 기법, 활성화 함수로는 softmax, 손실 함수로는 cross entropy를 사용
- Input layer : 특정 시점에서의 입력 단어에 대한 one-hot vector로 입력 받아 입력층의 가중치 행렬을 거쳐 embedding vector를 출력, NNLM과의 차이로는 window로 입력받아 한번에 처리된 lookup table과는 달리, 단어 입력 시점에 따라 입력층의 가중칠 행렬의 변화가 발생하기에 같은 단어에 대한 embedding vector 값이 다를 수 있음
- Embedding layer(linear) : Projection layer(투사층)의 역할로, 결과로 얻는 벡터를 Embedding vector 라고 함.
- Hidden layer(non-linear) : 이전 시점의 hidden state를 가중치와 편향을 사용한 선형 결합된 값을 출력하고 이를 활성화 함수(hyperbolic tansent)를 거쳐 현 시점의 hidden state를 생성, 다음 시점의 은닉층의 연산에 사용하거나 출력층으로 전달
- Output layer : 전달 받은 은닉 상태와 출력층의 가중치를 사용하여 이를 활성화 함수; softmax를 거쳐 각 원소의 값을 0~1 사이의 실수값을 가지며 총 합은 1이 되는 상태로 변환, 실제값에 해당하는 값과 cross-entropy 손실함수를 사용하여 역전파 실시

```python
# Preprocessing
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

text = """경마장에 있는 말이 뛰고 있다\n
그의 말이 법이다\n
가는 말이 고와야 오는 말이 곱다\n"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
vocab_size = len(tokenizer.word_index) + 1
print("Vocabulary size : %d" %vocab_size) # Vocabulary size : 12

sequences = list()
for line in text.split('\n'): # \n을 기준으로 문장 토큰화
    encoded = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(encoded)):
        sequence = encoded[:i+1]
        sequences.append(sequence)
print("Number of samples for training : %d" %len(sequences)) # Number of samples for training : 11

max_len = max(len(s) for s in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이

sequences = pad_sequences(sequences, maxlen=max_len, padding='pre') # label로 사용될 단어들의 길이를 padding을 통해 일치

# label 분리
sequences = np.array(sequences)
X = sequences[:, :-1]
y = sequences[:,-1]

y = to_categorical(y, num_classes=vocab_size)
```

---

### Word Embedding
- 자연어 처리의 성능에 크게 영향을 미치는 단어(텍스트)를 숫자로 변환하는 과정;벡터화에서 인공 신경망 학습을 통해 단어를 밀집 표현으로 변환하는 방법
- 단어를 밀집 벡터의 형태로 표현하는 방법

### Sparse Representation(희소 표현)
- one-hot-encoding과 같이 대부분의 값이 0으로 표현되는 방법으로 단어의 개수가 늘어나면서 벡터의 차원이 지나치게 커져 공간적 낭비를 일으킴
- 벡터간의 유의미한 유사성을 표현할 수 없기에, Disributed representation(분산 표현)을 사용해 다차원 공간에 단어의 의미를 벡터화하여 유사성을 벡터화하는 embedding을 사용
    * 분산 표현 : 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다는 가정(분포 가설) 하에 주로 함께 등장하는 단어들의 벡터들에 대해 학습하고 의미를 저차원에 벡터의 여러 차원에 분산하여 표현

### Dense Representation(밀집 표현)
- 벡터의 차원을 단어 집합의 크기로 상정하지 않고 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 지정하여 실수값으로 표현

### Word2Vec(Word to Vector)
- 원-핫 벡터와는 달리 단어 벡터 간의 유사도를 반영하여 단어를 수치화하는 방법
- 다음 단어를 예측하는 NNLM과 달리 Word2Vec은 워드 임베딩을 목적으로 중심 단어를 예측하여 학습하므로 예측 전, 후의 단어들을 모두 참고
- 은닉층을 제거하여 투사층 다음에 바로 출력층으로 연결하기에 연산량이 줄어 학습속도의 이점
    * Hierarchical softmax(계층적 소프트맥스)
    * Negative sampling
- CBOW(Continuous Bag of Words) : 주변에 있는 단어들을 입력으로 투사층에서 입력 단어 벡터들의 lookup table 평균(input vector * `W`)과 출력층에서의 `W'`을 통해 중간에 있는 단어를 예측
    * Center word : 예측해야 할 단어
    * Context word : 예측에 사용되는 단어
    * Window : 중심 단어를 예측하기 위해서 참고할 단어의 범위로 window=n일 때, center word를 중심으로 앞 뒤의 n개, 총 2n개의 단어를 참고
    * Sliding window : 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가면서 학습을 위한 데이터 셋을 만드는 방법
- Skip-Gram : 중간에 있는 단어를 입력으로 주변 단어 `2n`개를 예측

### FastText
- 단어 안에도 여러 단어들이 존재하는 것으로 간주
- Subword를 고려하여 학습; 단어를 N-gram의 구성으로 취급하여 내부단어 토큰을 벡터화
- Dataset 내부의 모든 단어의 n-gram이 실시되기에, dataset 양이 충분할 경우 Out Of Vocabulary(OOV)에 대해서도 다른 단어와의 유사도를 계산 가능
- 등장 빈도 수가 적은 단어(rare words)에 대해 비교적 높은 임베딩 벡터값을 얻음
- 노이즈가 많은 코퍼스에서 강점을 가짐; 훈련 코퍼스에 오타나 맞춤법이 틀린 단어가 있을 경우의 임베딩 가능

```python
from gensim.models import FastText

model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)

model.wv.most_similar("electrofishing")
```

```bash
[('electrolux', 0.7934642434120178), ('electrolyte', 0.78279709815979), ('electro', 0.779127836227417), ('electric', 0.7753111720085144), ('airbus', 0.7648627758026123), ('fukushima', 0.7612422704696655), ('electrochemical', 0.7611693143844604), ('gastric', 0.7483425140380859), ('electroshock', 0.7477173805236816), ('overfishing', 0.7435552477836609)]
```

- 한국어에서의 FastText
    * 음절 단위
    * 자모 단위(초성, 중성, 종성)

### Pre-trained Word Embedding
- Keras에서의 Embedding() : 인공 신경망 구조 관점에서 embedding layer를 제공
    * Embedding Layer는 look-up table(=weight matrix) : 입력 시퀀스의 각 단어들은 모두 정수 인코딩(index)되어 임베딩 층을 거쳐 밀집 벡터(dense vector=embedding vector)로 mapping, 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식(역전파)으로 훈련
    * Embedding layer 사용
    
    ```python
    # Preprocessing
    import numpy as np
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences

    sentences = ['nice great best amazing','stop lies','pitiful nerd','excellent work','supreme quality','bad','highly respectable']
    y_train = [1,0,0,1,1,0,1]

    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(sentences)
    vocab_size = len(tokenizer.word_index) + 1

    X_encoded = tokenizer.texts_to_sequences(sentences)
    print('Encoding to integer : ', X_encoded)
    # 정수 인코딩 결과 : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]

    max_len = max(len(l) for l in X_encoded) # 샘플의 최대 길이
    X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post') # post(뒤)에 패딩
    y_train = np.array(y_train)
    ```
    
    ```bash
    패딩 결과 :
    [[ 1  2  3  4]
    [ 5  6  0  0]
    [ 7  8  0  0]
    [ 9 10  0  0]
    [11 12  0  0]
    [13  0  0  0]
    [14 15  0  0]]
    ```
    
    ```python
    # Binary Classification Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, Dense, Flatten

    embedding_dim = 4
    
    model = Sequential()
    model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
    model.fit(X_train, y_train, epochs=100, verbose=2)
    ```

- Pre-trained word embbedding
    * GloVe
    * Word2Vec

### Embeddings from Language Model;ELMo
- 언어 모델로 하는 임베딩으로 사전 훈련된 언어 모델을 활용
- 완전히 다른 의미를 갖는 같은 단어에 대해 기존의 모델들의 임베딩 벡터들은 제대로 반영 불가
- 같은 표기의 단어라도 문맥에 따라서 다르게 워드 임베딩을 할 수 있음(Contextualized Word Embedding)

### Bidirectional Language Model;biLM
- 은닉층이 최소 2개 이상의 다층 구조를 가지며 토큰화된 문장으로 CNN을 이용한 문자 임베딩을 통해 얻은 단어 벡터를 입력, 순방향과 역방향의 언어모델을 사용
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/biLM.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

1. 각 층의 출력값을 concatenate
2. 각 층의 출력값 별로 가중치 부여
3. 각 층의 출력값의 합을 출력
4. 벡터의 크기를 결정하는 스칼라 매개 변수를 곱하여 ELMo representation을 도출
- 양방향 RNN에서는 순방향의 Hidden State과 역방향의 Hidden State을 concatenate하여 다음층의 입력으로 사용하고, biLM은 각각을 별개의 모델로 학습

### BIO Representation
- Named Entity Recognition(개체명 인식) : chatBoT 등에서 필요한 주요 전처리 작업으로, 직접 목적에 맞는 데이터를 준비하여 모델을 생성하는 것이 도메인 또는 목적에 특화한 성능을 보임
- BIO Tagging : 개체명 인식에서 corpus로부터 개체명을 인식하기 위한 방법 중 하나로, 여러 종류의 개체에 대해 어떤 종류에 속하는 지도 함께 태깅(B:Begin=개체명이 시작되는 부분, I:Inside=개체명의 내부 부분, O:Outside=개체명이 아닌 부분)
