---
title: 
author: Patrick
date: 2024-01-15 13:18:00 +0900
tags: []
categories: []
render_with_liquid: false
---
## Paper
- Optical Character Recognition; OCR(광학 문자 인식)

### Summary
한글 OCR 성능을 높이기 위해 딥 러닝 모델을 활용하여 문자 인식 부분을 개선하고자 하였다.
본 논문에서는 폰트와 사전데이터를 사용해 딥러닝 모델 학습을 위한 한글 문장 이미지 데이터를 직접 생성해보고 이를 활용해서 한글 문장의 OCR 성능을 높일 다양한 모델 조합들 에 대한 실험을 진행했다.
딥러닝 모델은 STR(Scene Text Recognition) 구조를 사용해 변환, 추출, 시퀀스, 예측 모듈 각 24가지 모델 조합을 구성했다.
딥러닝 모델을 활용한 OCR 실험 결과 한글 문장에 적합한 모델조합은 변환 모듈을 사용하고 시퀀스와 예측 모듈에는 BiLSTM과 어텐션을 사용한 모델조합이 다른 모델 조합에 비해 높은 성능을 보였다.
해당 논문에서는 이전 한글 OCR 연구와 비교해 적 용 범위를 글자 단위에서 문장 단위로 확장하였고 실제 문서 이미지에서 자주 발견되는 유형의 데이터를 사용해 애플리케이션 적용 가능 성을 높이고자 한 부분에 의의가 있다.
#### Problems
1. 손 글씨 및 서명과 같은 비 정형화되어 있는 데이터
2. 배경과 문자의 구분의 어려움
3. 노이즈, 왜곡, 밀도, 저 해상도 등의 간섭 요소로 인한 식별의 어려움

#### Korean issue
- 영문과 비교해 분류해야 할 글자 수가 35 배정도 차이를 보여 정확도 상승에서 한계(알파벳 26 종 : 한글 문자 11172개 중 주로 사용하는 문자 900개)
- 기존의 글자 단위 탐지에서 단일 모델(CNN, RNN)의 score map. afinity map을 통해서 문자 영역 탐지와 여백 분류를 통한 단어 분류가 가능함을 확인
- 문자 인식 부분에 더욱 집중하여  연구를 진행
### OCR 구성
#### 1. Text detection(문자 탐지)
- 기존 모델 : 모델 학습 중의 튜닝의 어려움, 모델의 느린 속도로 인한 **실시간 탐지 적용의 어려움**
	- 글자/단어 후보 생성
	- 후보 필터링
	- 그룹화
- SSD 논문에서 제시된 **Textboxes** : 문자와 비문자 경계 박스에 대한 회귀 분석 방식을 활용하여 이미지 간 간격이 좁아 구별의 어려움
	- Semantic Segmentation(의미 분할) : 분할의 기본 단위를 클래스로 하여 같은 클래스에 해당하는 사물을 예측하고 마스크 상에 동일한 색상으로 표시하여 위치 회귀 없이 텍스트 박스에서 분할 결과를 직접 추출(ex - PixelLink의 instance segmentation)
	- Fast Oriented Text Spotting; FOTS : End-to-end(종단 간) 접근 방식을 적용해 탐지와 인식 모듈이 동시에 훈련됨으로써 상승한 인식 결과가 다시 탐지 모듈의 정확도를 높이는 효과
#### 2. Text recognition(문자 인식)
- 특성을 추출하는 CNN과 시계열 모델인 RNN을 통합하여 하나의 통일된 네트워크 구조의 CRNN을 제안
- CNN을 통해 입력된 이미지로부터 특성 시퀀스를 추출하고 이를 RNN의 입력값으로 하여 이미지의 텍스트 시퀀스를 예측, 결과로 출력
- Gated RCNN은 recurrent convolution layer에 있는 context module을 제어하기 위해 게이트를 더한 모델로, 게이트는 모듈 제어 뿐만 아니라 CNN의 정보와 RNN의 정보를 균형 있게 제어하는 역할을 수행
- Attention drift : 특성 영역과 타깃 사이의 정확한 alignment(조정)를 얻지 못하는 현상
- Focusing Attention Network; FAN : Attention drift를 해결하기 위해 고안된 방법
	1. Attention network는 글자 타깃을 인식하는 데 사용
	2. Focusing network는 Attention network가 타깃 지역에 적절히 Attention을 갖는지 측정하여 Attention을 조절
#### 3. End-to-end(종단 간) 문자 인식
- Attentional Scene Text Recognizer with Flexible Rectification; ASTER : 교정 네트워크와 인식 네트워크를 종단 간으로 연결하는 모델
	- 교정 네트워크 : 입력 이미지를 새로운 이미지로 adaptive 변환하여 이미지 속 기울어지거나 왜곡된 텍스트를 교정
	- 인식 네트워크 : Attention 기반의 seq2seq 모델로 변환된 이미지로부터 글자 sequence를 예측
- TextSpotter : Text-alignment layer와 character attention 메커니즘을 사용하여 통합 구조를 생성, 문자 탐지와 문자 인식 두 개 모듈의 통합
- 서로의 convolutional feature를 공유함으로써 상호보완적으로 작동하고 종단 간으로 훈련이 가능하여 성능이 향상
- Tesseract OCR Engine : 상업용 OCR 엔진의 성능을 개선하기 위해 HP사 연구소에서 제작된 Open source OCR로, LSTM 기반 버전까지 출시되어 100개 이상의 언어를 지원하나 한글에 대한 성능은 정확도가 낮아 실제 적용에 어려움


Score map, Affinity map
Bounding box를 사용해 인식한 문자 영역을 표시
글자 영역 탐지보다 탐지한 글자의 인식을 중점적으로 실험을 진행
### Scene Text Recognition; STR 모델 구조 - 4가지 모듈로 구성
#### 1. Transform(변환)
- Thin Plate Spline; TPS 변환 사용 여부에 따라 모듈이 구분
	- TPS 변환 :  데이터 보간 및 평활화(기울어짐, 왜곡)를 위한 spline 기반 기술
	- Ref. https://blog.naver.com/anhyung/222361619933
#### 2. Extraction(추출)
- 이미지에서 시각적 특성을 추출하는 부분
- VGGNet, RCNN, ResNet 모델을 사용하여 구성
	- VGGNet : 간단한 구조로 고성능의 이미지 특성 추출
	- RCNN :  이미지 분류를 수행하는 CNN과 이미지에서 객체가 존재할 영역을 제안해주는 region proposal algorithm을 연결
	- ResNet : 망이 깊어지는 경우 발생하는 Gradient vanishing, explosion을 방지하기 위해 layer의 입, 출력을 연결하는 skip connetion을 사용
#### 3. Sequence
- BiLSTM 사용 여부에 따라 모듈이 구분
#### 4. Predict(예측)
- Connetionist Temporal Classification; CTC와 Attention으로 모듈이 구분
	- CTC : Train data에 클래스 label만 순서대로 있고, 각 클래스의 위치 정보는 포함되지 않으며 분할 되지 않은 sequence data의 학습을 위해서 사용하는 알고리즘
- 문장 길이가 길고 층이 깊으면, encoder가 압축해야 할 정보가 많아 정보 손실이 일어나고, decoder는 압축된 정보를 초반 예측에만 사용하는 경향을 보이는데, Attention 모듈을 통해 decoder의 예측에서 가장 의미 있는 encoder의 입력에 주목


### Code
#### Modules
- 1. `transformation.py`
	- 
- 2. `feature_extraction.py`
	- 
- 3. `sequence_modeling.py`
	- 
- 4. `prediction.py`

#### model.py
- `__init__()`

```python
def __init__(self, opt):
	super(Model, self).__init__()
	self.opt = opt
	self.stages = {'Trans': opt.Transformation, 'Feat': opt.FeatureExtraction,
				   'Seq': opt.SequenceModeling, 'Pred': opt.Prediction}
				   
	""" Transformation : TPS 여부"""
	if opt.Transformation == 'TPS':
		self.Transformation = TPS_SpatialTransformerNetwork(
			F=opt.num_fiducial, I_size=(opt.imgH, opt.imgW), I_r_size=(opt.imgH, opt.imgW), I_channel_num=opt.input_channel)
	else:
		print('No Transformation module specified')

	""" FeatureExtraction """
	if opt.FeatureExtraction == 'VGG':
		self.FeatureExtraction = VGG_FeatureExtractor(opt.input_channel, opt.output_channel)
	elif opt.FeatureExtraction == 'RCNN':
		self.FeatureExtraction = RCNN_FeatureExtractor(opt.input_channel, opt.output_channel)
	elif opt.FeatureExtraction == 'ResNet':
		self.FeatureExtraction = ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)
	else:
		raise Exception('No FeatureExtraction module specified')
	self.FeatureExtraction_output = opt.output_channel  # int(imgH/16-1) * 512
	self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1

	""" Sequence modeling"""
	if opt.SequenceModeling == 'BiLSTM':
		self.SequenceModeling = nn.Sequential(
			BidirectionalLSTM(self.FeatureExtraction_output, opt.hidden_size, opt.hidden_size),
			BidirectionalLSTM(opt.hidden_size, opt.hidden_size, opt.hidden_size))
		self.SequenceModeling_output = opt.hidden_size
	else:
		print('No SequenceModeling module specified')
		self.SequenceModeling_output = self.FeatureExtraction_output

	""" Prediction """
	if opt.Prediction == 'CTC':
		self.Prediction = nn.Linear(self.SequenceModeling_output, opt.num_class)
	elif opt.Prediction == 'Attn':
		self.Prediction = Attention(self.SequenceModeling_output, opt.hidden_size, opt.num_class)
	else:
		raise Exception('Prediction is neither CTC or Attn')

```

```python
class Model(nn.Module):
    
  
    def forward(self, input, text, is_train=True):

        """ Transformation stage """

        if not self.stages['Trans'] == "None":

            input = self.Transformation(input)

  

        """ Feature extraction stage """

        visual_feature = self.FeatureExtraction(input)

        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]

        visual_feature = visual_feature.squeeze(3)

  

        """ Sequence modeling stage """

        if self.stages['Seq'] == 'BiLSTM':

            contextual_feature = self.SequenceModeling(visual_feature)

        else:

            contextual_feature = visual_feature  # for convenience. this is NOT contextually modeled by BiLSTM

  

        """ Prediction stage """

        if self.stages['Pred'] == 'CTC':

            prediction = self.Prediction(contextual_feature.contiguous())

        else:

            prediction = self.Prediction(contextual_feature.contiguous(), text, is_train, batch_max_length=self.opt.batch_max_length)

  

        return prediction
```