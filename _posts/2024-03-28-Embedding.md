---
title: 
author: Patrick
date: 2024-03-28 13:18:00 +0900
tags: []
categories: []
render_with_liquid: false
---
## Embedding
- 임베딩(embedding)은 고차원의 복잡한 데이터를 저차원의 공간으로 변환하는 기법입니다. 
- 딥러닝과 자연어 처리 분야에서 자주 사용되며, 텍스트, 이미지, 오디오 등 다양한 유형의 데이터를 효과적으로 표현하고 처리하기 위해 사용됩니다. 
- 임베딩은 데이터의 중요한 특징이나 구조를 보존하면서 차원을 축소하는 방식으로 동작합니다.

### 텍스트 임베딩

- **Word Embedding**: 단어를 고차원의 벡터로 변환합니다. 예를 들어, Word2Vec, GloVe, FastText 등이 있습니다.
- **Sentence Embedding**: 문장이나 문단을 벡터로 표현합니다. BERT, ELMo, Universal Sentence Encoder 등이 여기에 해당합니다.

### 이미지 임베딩

- **Feature Extraction**: CNN(Convolutional Neural Network)을 사용하여 이미지에서 중요한 특징이나 패턴을 추출합니다.
- **Transfer Learning**: 사전 훈련된 이미지 모델(예: VGG, ResNet)을 사용하여 다양한 이미지 분류나 태스크에 적용합니다.

### 오디오 임베딩

- **Spectrogram**: 오디오 신호를 주파수와 시간 정보를 가진 이미지 형태인 스펙트로그램으로 변환하여 처리합니다.
- **MFCC (Mel-Frequency Cepstral Coefficients)**: 오디오에서 중요한 특징을 추출하여 임베딩합니다.

### 임베딩의 장점

- **차원 축소**: 고차원의 데이터를 저차원으로 축소하여 계산 효율성을 높입니다.
- **특징 추출**: 데이터의 중요한 특징이나 패턴을 강조하여 학습 및 분류 성능을 향상시킵니다.
- **유사성 캡처**: 벡터 공간에서 유사한 항목들이 서로 가깝게 위치하게 하여 의미적 유사성을 캡처합니다.

### 임베딩의 한계

- **정보 손실**: 차원 축소 과정에서 일부 정보가 손실될 수 있습니다.
- **과적합**: 고차원 데이터를 저차원으로 축소할 때 너무 단순화되어 모델이 과적합될 수 있습니다.

#### torch.nn.Embedding(num_embeddings, embedding_dim)
- `num_embeddings`과 `embedding_dim`을 정하는 기준은 다양한 요소와 상황에 따라 결정
1. **데이터의 크기와 복잡성**
    - `num_embeddings`*(int)*: 사용하려는 고유한 항목(예: 어휘의 크기, 카테고리의 수)의 총 개수입니다. 이 값은 데이터셋의 다양성과 복잡성에 따라 달라집니다.
    - `embedding_dim`*(int)*: 임베딩 벡터의 차원입니다. 일반적으로 이 값은 크게 설정될수록 모델의 표현 능력이 증가하지만, 계산 비용이 높아질 수 있습니다.
2. **컴퓨팅 리소스**
    - `embedding_dim`이 높을수록 연산량이 증가하므로, 사용 가능한 컴퓨팅 리소스를 고려하여 적절한 값을 선택해야 합니다.
3. **모델의 성능과 복잡성**
    - 임베딩 벡터의 차원이 클수록 모델은 더 복잡한 패턴과 관계를 학습할 수 있습니다. 하지만, 이는 과적합의 위험도 증가시킬 수 있으므로 조심스럽게 설정해야 합니다.
4. **이전 연구나 벤치마크**
    - 유사한 작업이나 데이터셋에서 얻은 성과를 참고하여 `num_embeddings`과 `embedding_dim`의 초기 값을 설정할 수 있습니다.
5. **하이퍼파라미터 튜닝**
    - 초기 모델 학습 후 성능을 평가하고, 필요한 경우 `num_embeddings`과 `embedding_dim`을 조절하여 모델의 성능을 향상시킬 수 있습니다.

---
### Word Embedding
  
- 자연어 처리(NLP)에서 사용되는 워드 임베딩 기법은 텍스트 데이터의 단어를 고차원 벡터로 표현하는 방법입니다. 
- 워드 임베딩은 단어 간의 의미적, 구문적 관계를 캡처하며, 다양한 NLP 작업에서 성능 향상을 가져옵니다.

1. **Word2Vec**:
    - 앞서 언급한 Word2Vec은 단어의 분산 표현을 학습하는 기법입니다.
    - 주로 Skip-gram과 CBOW 두 가지 구조를 사용하며, 대량의 텍스트 데이터에서 효과적으로 단어의 벡터 표현을 학습합니다.
2. **GloVe (Global Vectors for Word Representation)**:
    - GloVe는 단어의 동시 출현 통계를 기반으로 단어 간의 의미적 유사성을 캡처하는 방법입니다.
    - 전체적인 통계 정보를 활용하여 단어 간의 관계를 학습하므로, Word2Vec과 유사한 성능을 보이며 대규모 데이터에 특히 유용합니다.
3. **FastText**:
    - FastText는 Facebook Research에서 개발한 워드 임베딩 방법으로, 단어를 서브워드(subword) 단위로 분해하여 학습합니다.
    - 이로 인해 희소한 단어나 오타가 포함된 단어에 대해서도 강건하게 임베딩을 학습할 수 있습니다.
4. **BERT (Bidirectional Encoder Representations from Transformers)**:
    - BERT는 Transformer 아키텍처를 기반으로 하는 사전 훈련된 언어 모델입니다.
    - 양방향으로 문맥을 이해하여 단어나 문장의 의미를 효과적으로 임베딩합니다.
    - 최근에는 BERT를 기반으로 다양한 자연어 처리 작업에 적용되는 파인 튜닝(fine-tuning) 방법이 주목받고 있습니다.
5. **ELMo (Embeddings from Language Models)**:
    - ELMo는 단어의 의미를 여러 레이어의 언어 모델을 통해 학습하는 방법입니다.
    - 단어의 다양한 문맥을 고려하여 임베딩을 생성하므로, 다의성(disambiguation) 문제에 효과적입니다.

---
### Word2Vec

- Word2Vec은 자연어 처리(NLP)와 텍스트 마이닝 분야에서 널리 사용되는 word embedding 기법
- 이 기법은 단어를 고차원 벡터로 표현하여 단어 간의 의미적 유사성을 캡처
- Word2Vec 모델은 대량의 텍스트 데이터에서 단어의 분산 표현(distributed representation)을 학습하는 데 효과적입니다.

- **의미적 유사성 캡처**: Word2Vec은 단어 간의 의미적 유사성을 캡처하는 데 효과적입니다. 예를 들어, 유사한 의미를 가진 단어들이 벡터 공간에서 서로 가까이 위치하게 됩니다.
- **효율적인 벡터 표현**: Word2Vec은 각 단어를 고정된 차원의 밀집 벡터로 표현하므로, 메모리 사용량이 적고 계산이 빠릅니다.
- **전이 학습**: Word2Vec은 사전 훈련된 임베딩을 사용하여 다양한 자연어 처리 작업에 전이 학습(transfer learning)을 적용할 수 있습니다.
#### Skip-gram
- 주어진 단어로 주변의 단어를 예측하는 방식입니다.
- 예를 들어, "apple"이라는 단어가 주어졌을 때 "juicy", "red", "fruit" 등의 주변 단어를 예측하는 방식입니다.
#### CBOW (Continuous Bag of Words)
- 주변 단어들을 통해 중앙의 단어를 예측하는 방식입니다.
- 예를 들어, "juicy", "red", "fruit"과 같은 주변 단어들이 주어졌을 때 "apple"이라는 중앙 단어를 예측하는 방식입니다.
