---
title: Concept of Recurrent Neural Network
author: Patrick
date: 2023-11-05 18:40:00+0900
tags: [Concept]
categories: [NLP]
render_with_liquid: false
---
## Recurrent Neural Network; RNN
### RNN(순환 신경망)
- 입력과 출력을 Sequence 단위로 처리하는 인공 신경망 모델
	- FFNN : 은닉층에서 활성화 함수를 거친 값이 출력층 방향으로만 진행
	- RNN : 은닉층에서 활성화 함수(`tanh`를 주로 사용)를 거친 값이 출력층 혹은 은닉층 노드로 진행<br>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240107170727.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240107171654.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

#### Memory Cell; RNN Cell
- 은닉층에서 activation function을 거친 값을 내보내면서 이전 시점의 값을 기억하고 이를 입력으로 사용하는 노드
- Hidden State : t시점의 메모리 셀이 t+1 시점으로 보내는 값<br>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/Pasted%20image%2020240107171339.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- RNN Architecture : 각 시점에서 입력과 출력이 동시 혹은 하나만 이루어지는 지에 따라 모델의 종류(구조)가 변함<br>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/nlp/rnn_models.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

|structure|example|
|------|-------------|
|One-to-Many|Image Captioning|
|Many-to-One|Sentiment Classification, Spam Detection|
|Many-to-Many|Chatbot, Translator, Tagging task|

#### RNN Code with Keras and Python

```python
# Keras를 사용하여 RNN 생성
from tensorflow.keras.layers import SimpleRNN

# 추가 인자를 활용한 RNN layer 생성
model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim)))

# model.add(SimpleRNN(hidden_units, input_length=M, input_dim=N)) # 다른 표기
```

- hidden_units : 은닉 상태의 크기(RNN의 용량)을 정의; 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(ouptut_dim)
- Input : 3D tensor로 입력
    * batch_size : 한번에 학습하는 데이터의 수; sample의 수
    * input_dim : 입력 벡터의 차원 수;입력 단어 하나의 길이
    * timesteps : 입력 시퀀스의 길이(input_length);각 시점에서 단어 하나를 입력으로 받기에, 단어의 모음인 시퀀스의 길이라고 할 수 있음
- Output : return_sequences의 값에 따라 2D, 3D 결정
    * batch_size
    * output_dim : 최종 시점의 은닉 상태인 출력 벡터의 차원 수;출력 단어 하나의 길이
    * timesteps : 메모리 셀의 각 시점의 은닉 상태값(hidden states)

```python
# Python으로 직접 RNN 구성
import numpy as np

timestpes = 10 # Sequence의 길이
input_dim = 4 # 단어 벡터의 차원
hidden_units = 8 # 은닉 상태의 크기 = 메모리 셀의 용량

inputs = np.random.random((timestpes, input_dim))
hidden_state_t = np.zeros((hidden_units,)) # 초기 은닉 상태 초기화
print("Initial Hidden States :", hidden_state_t)

Wx = np.random.random((hidden_units, input_dim)) # 입력에 대한 가중치 2D 텐서 생성
Wh = np.random.random((hidden_units, hidden_units)) # 은닉 상태에 대한 가중치 2D 텐서 생성
b = np.random.random((hidden_units,)) # bias 생성

print("Wx, Wh, b : {}\n\n{}\n\n{}\n\n".format(Wx, Wh, b))
print("Wx, Wh, b shapes : ",np.shape(Wx), np.shape(Wh), np.shape(b))
print("\n")

total_hidden_states = [] # 모든 시점의 은닉 상태

# dot(arr1, arr2) : 벡터 계산 (arr1, arr2의 원소값들의 곱의 합)
for input_t in inputs:
    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t))
    total_hidden_states.append(list(output_t))

total_hidden_states = np.stack(total_hidden_states, axis=0)
print(total_hidden_states)
```

```bash
Initial : [0. 0. 0. 0. 0. 0. 0. 0.]
Wx, Wh, b : [[0.85143664 0.97714713 0.65502162 0.28371238]
 [0.30802615 0.09059362 0.1076857  0.25372438]
 [0.28704896 0.60082044 0.75228122 0.68312046]
 [0.68097992 0.80925337 0.0645649  0.12324691]
 [0.38990914 0.92962164 0.10098862 0.67962663]
 [0.94462422 0.01337485 0.03662331 0.03050986]
 [0.28653855 0.32739224 0.49828274 0.82087065]
 [0.96042989 0.83653596 0.07487261 0.09113481]]

[[0.68096678 0.58803731 0.57988288 0.92890096 0.09135938 0.99057804
  0.01872872 0.4546827 ]
 [0.13087967 0.76399839 0.56236402 0.69592914 0.36460709 0.72990083
  0.45858138 0.00552776]
 [0.55843321 0.64096323 0.53645843 0.79040003 0.42508579 0.30223451
  0.5627866  0.86003013]
 [0.4413374  0.97321599 0.84578887 0.25420944 0.73637994 0.391908
  0.17252162 0.76213319]
 [0.06361856 0.73890545 0.58717524 0.45134905 0.05404353 0.4730816
  0.55602838 0.81121357]
 [0.56893496 0.79008566 0.87842282 0.00705078 0.06712543 0.27438764
  0.40791329 0.50191709]
 [0.18485617 0.67680875 0.32107997 0.67353141 0.58586511 0.80557961
  0.9308333  0.33228672]
 [0.58482391 0.67038387 0.61891013 0.40433411 0.7752712  0.49077138
  0.22612616 0.23695048]]

[0.77085078 0.46451979 0.05440115 0.82049665 0.09553835 0.78358678
 0.47607443 0.87411462]


Wx, Wh, b shapes :  (8, 4) (8, 8) (8,)


[[0.9311158  0.28819712 0.81778765 0.79569775 0.80725949 0.42711984
  0.64165978 0.84558929]
 [0.93584739 0.33438469 0.79853643 0.8403642  0.84959694 0.54169853
  0.64857779 0.88908515]
 [0.89903488 0.45231785 0.85710553 0.67561648 0.77069602 0.63738225
  0.81731435 0.77063852]
 [0.75156127 0.33641803 0.63492503 0.51016977 0.51385085 0.59919868
  0.59905286 0.63723528]
 [0.88905473 0.44012962 0.8013277  0.70813697 0.75661072 0.68661386
  0.76037359 0.80598781]
 [0.95834118 0.44270473 0.89593064 0.82501953 0.8508026  0.65423909
  0.80873311 0.88625939]
 [0.93018297 0.50930848 0.90068338 0.78150259 0.8930009  0.63937644
  0.8779043  0.84691315]
 [0.82701004 0.24378468 0.78295419 0.45492988 0.48932459 0.31174412
  0.63485924 0.52788644]
 [0.9632202  0.51884557 0.91992141 0.85340389 0.90910772 0.70188364
  0.87507954 0.90729884]
 [0.89504751 0.38634984 0.83208148 0.62738128 0.65260641 0.60001641
  0.74679467 0.73196294]]
```

### Deep RNN(깊은 순환 신경망)
- 은닉층의 개수(깊이)가 복수인 경우

#### Code

```python
model = Sequential()
# 첫번째 은닉층을 정의할 때, 각 시점의 값을 다음 은닉층으로 값을 전달하기 위해 retrun_sequences 값을 True로 설정
model.add(SimpleRNN(hidden_units, input_length=10, input_dim=5, return_sequences=True))
model.add(SimpleRNN(hidden_units, return_sequences=True))
```

### Bidirectional RNN(양방향 RNN)
- 시점 t의 출력값을 예측할 때, t 이전 시점의 입력뿐만 아니라, 이후 시점의 입력 또한 예측에 기여할 수 있다는 아이디어 기반 RNN
- 기본적으로는 두 개의 메모리 셀을 사용하여 첫 번째에서는 Forward States를, 두 번째에서는 Backward States를 전달 받아 Hidden State를 계산

---

## RNN Text Classification
- Binary Classification
- Multi-Class Classification
### Text Classification using Keras(supervised learning)
- Supervised Learning(지도 학습) : Train Data로 Label이라는 정답을 포함하고 있는 Dataset을 사용하여 학습; 정답을 알고 있는 상태로 훈련
- Validation(검증) : 모든 Sample을 사용하여 학습하지 않고 일정 비율의 데이터를 남기고 이를 예측한 뒤 Label 값과 대조하여 검증하는 데 사용
- Embedding : 각각의 단어가 정수로 변환된 값(index)를 입력으로 받아 임베딩 작업을 수행; 인덱싱 작업(정수 인코딩)의 방법 중 하나로는 빈도수에 따라 정렬
- 텍스트 분류는 RNN의 Many-to-one 문제에 속하므로, 모든 시점에 대해서 입력을 받지만, 최종시점의 은닉 상태 만을 출력
    * Binary Classification : 출력값의 종류가 두 종류일 경우(loss function = binary_crossentropy)
    * Multi-Class Classification : 출력값의 종류가 세 가지 이상(loss function : categorical_crossentropy)

```python
model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim)))
# hidden_units : RNN 출력의 크기; 은닉 상태의 크기
# timesteps : 시점의 수; 각 문서의 단어 수
# input_dim : 입력의 크기; 임베딩 벡터의 차원
```

### Tagging Task using Keras
- 지도 학습을 통해 이루어지는 분류 작업
#### Named Entity Recognition(개체명 인식)
- 이름(의미)을 갖는 개체를 보고 해당 단어(개체)의 유형 파악 

```python
from nltk import word_tokenize, pos_tag, ne_chunk

sentence = "James is working at Disney in London"

# Tokenize the sentence and tag
tokenized_sentence = pos_tag(word_tokenize(sentence))
print(tokenized_sentence)

# 개체명 인식
ner_sentence = ne_chunk(tokenized_sentence)
print(ner_sentence)
```

#### Part-of-Speech Tagging(품사 태깅) 
- 단어의 품사 파악      

```python
# Preprocessing
import nltk
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Tokenized and Tagged Data
tagged_sentences = nltk.corpus.treebank.tagged_sents()
print("품사 태깅이 된 문장 개수: ", len(tagged_sentences)) # 3914

# Diverse data into words and tags
sentences, pos_tags = [], [] 
for tagged_sentence in tagged_sentences:
	sentence, tag_info = zip(*tagged_sentence) # as the tagged_sentence shows tuple of word and tag of each sample, so use zip() to seperate them
	sentences.append(list(sentence))
	pos_tags.append(list(tag_info))

def tokenize(samples):
	tokenizer = Tokenizer()
	tokenizer.fit_on_texts(samples)
	return tokenizer

src_tokenizer = tokenize(sentences)
tar_tokenizer = tokenize(pos_tags)

vocab_size = len(src_tokenizer.word_index) + 1
tag_size = len(tar_tokenizer.word_index) + 1
print('단어 집합의 크기 : {}'.format(vocab_size))
print('태깅 정보 집합의 크기 : {}'.format(tag_size))

# Encoding
X_train = src_tokenizer.texts_to_sequences(sentences)
y_train = tar_tokenizer.texts_to_sequences(pos_tags)

# padding into max length of encoded samples
max_len = 150
X_train = pad_sequences(X_train, padding='post', maxlen=max_len)
y_train = pad_sequences(y_train, padding='post', maxlen=max_len)

# Generate POS Tagger
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding
from tensorflow.keras.optimizers import Adam

embedding_dim = 128
hidden_units = 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, mask_zero=True)) # Zero padding
model.add(Bidirectional(LSTM(hidden_units, return_sequences=True))) # Many-to-many
model.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))

model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=128, epochs=7, validation_data=(X_test, y_test))

# Testing 
index_to_word = src_tokenizer.index_word
index_to_tag = tar_tokenizer.index_word

i = 10 # 확인하고 싶은 테스트용 샘플의 인덱스.
y_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측값 y를 리턴
y_predicted = np.argmax(y_predicted, axis=-1) # 확률 벡터를 정수 레이블로 변환.

print("{:15}|{:5}|{}".format("단어", "실제값", "예측값"))
print(35 * "-")

for word, tag, pred in zip(X_test[i], y_test[i], y_predicted[0]):
	if word != 0: # PAD값은 제외함.
		print("{:17}: {:7} {}".format(index_to_word[word], index_to_tag[tag].upper(), index_to_tag[pred].upper()))
```